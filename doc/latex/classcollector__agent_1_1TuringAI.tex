\doxysection{collector\+\_\+agent.\+Turing\+AI Class Reference}
\hypertarget{classcollector__agent_1_1TuringAI}{}\label{classcollector__agent_1_1TuringAI}\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_af257234d0a4c96de907ebd9c694b445a}\label{classcollector__agent_1_1TuringAI_af257234d0a4c96de907ebd9c694b445a} 
{\bfseries \+\_\+\+\_\+init\+\_\+\+\_\+} (self)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_a10564f82ddc1142bf22a1c86244dea0b}{nb\+\_\+item}} (self, looked, item)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_ab58a7f359400374bb84c51476b9de022}{get\+\_\+state}} (self, response)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_ad0209978aa21f09800352db495c5e4b6}{check\+\_\+level\+\_\+up}} (self, box\+\_\+zero)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_a62495a46224207018b55804fb7e98768}{remember}} (self, state, action, reward, next\+\_\+state, done)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_a0ec7d8b21bb7c978c3e365b9d51e1ccc}{train\+\_\+long\+\_\+memory}} (self)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_af4930c63b53df4f4a801a7917b83e4c2}{train\+\_\+short\+\_\+memory}} (self, state, action, reward, next\+\_\+state, done)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_a2010244c6386da9ea421ec83f3a8f2e5}{get\+\_\+action}} (self, state)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_a45d9c5b9867ca718d43260b0714edbdd}{compute\+\_\+reward}} (self, conn, final\+\_\+move, result, elapsed\+\_\+time)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_acb4b0925a5102cd09048259c36c98a98}{add\+\_\+item}} (self, response, command)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_a1c57e9bb8659005fa7519995e588644e}{save\+\_\+information}} (self)
\item 
\mbox{\hyperlink{classcollector__agent_1_1TuringAI_a89ab9a530c27c52fc56021638410d5f3}{train}} (self, conn)
\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_a97e65069f5fc1fb20a8ee59af216cfb5}\label{classcollector__agent_1_1TuringAI_a97e65069f5fc1fb20a8ee59af216cfb5} 
{\bfseries debug}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_a0c6f21e50fe693c6b64a022218c951d3}\label{classcollector__agent_1_1TuringAI_a0c6f21e50fe693c6b64a022218c951d3} 
{\bfseries port}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_a4c4bacd04c5d81d49e06c28fdb04dcc8}\label{classcollector__agent_1_1TuringAI_a4c4bacd04c5d81d49e06c28fdb04dcc8} 
{\bfseries conn}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_a9176cf424c1b99edb61d14f7fe7913cf}\label{classcollector__agent_1_1TuringAI_a9176cf424c1b99edb61d14f7fe7913cf} 
{\bfseries team\+\_\+name}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_a3e9685925e633ea5282299916a47bea5}\label{classcollector__agent_1_1TuringAI_a3e9685925e633ea5282299916a47bea5} 
{\bfseries host}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_aede07ffe880423ba3995c2f7ff5d4666}\label{classcollector__agent_1_1TuringAI_aede07ffe880423ba3995c2f7ff5d4666} 
{\bfseries command}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_ac9125edbb648481ee29b99642639ca48}\label{classcollector__agent_1_1TuringAI_ac9125edbb648481ee29b99642639ca48} 
{\bfseries ngames}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_a1c9b00d71130eb79fafeb510d05ba6bd}\label{classcollector__agent_1_1TuringAI_a1c9b00d71130eb79fafeb510d05ba6bd} 
{\bfseries epsilon}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_a6cc6962f7853bcebb9a34a29d7d10862}\label{classcollector__agent_1_1TuringAI_a6cc6962f7853bcebb9a34a29d7d10862} 
{\bfseries gamma}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_aefe1ec7911cd7318762c47dbd4773787}\label{classcollector__agent_1_1TuringAI_aefe1ec7911cd7318762c47dbd4773787} 
{\bfseries memory}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_a63f327d46ae3a5166b125ffcf8b90f9e}\label{classcollector__agent_1_1TuringAI_a63f327d46ae3a5166b125ffcf8b90f9e} 
{\bfseries device}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_aaf5110e2101edfad402bd8dd00dcaead}\label{classcollector__agent_1_1TuringAI_aaf5110e2101edfad402bd8dd00dcaead} 
{\bfseries model}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_ae3bcdfecf2852595fe8f2a9a9ba080f7}\label{classcollector__agent_1_1TuringAI_ae3bcdfecf2852595fe8f2a9a9ba080f7} 
{\bfseries trainer}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_ac92f14026af14e8aafd9fa3623f8eac6}\label{classcollector__agent_1_1TuringAI_ac92f14026af14e8aafd9fa3623f8eac6} 
{\bfseries level}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_ae1ba3c819ec5435c86fd53098102f338}\label{classcollector__agent_1_1TuringAI_ae1ba3c819ec5435c86fd53098102f338} 
{\bfseries attempt}
\item 
\Hypertarget{classcollector__agent_1_1TuringAI_aeac95c587b06b32636acde7b639b0cf1}\label{classcollector__agent_1_1TuringAI_aeac95c587b06b32636acde7b639b0cf1} 
{\bfseries inventory}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}The TuringAI class represents an AI agent for playing the game Zappy.
It contains methods for training and making decisions based on the game state.

Attributes:
    debug (bool): Indicates whether debug mode is enabled.
    port (int): The port number for the Zappy server.
    conn (Connection): The connection object for communicating with the server.
    team_name (str): The name of the team the AI agent belongs to.
    host (str): The hostname or IP address of the Zappy server.
    command (list): The list of available commands that the AI agent can perform.
    ngames (int): The number of games played by the AI agent.
    epsilon (int): The exploration rate for choosing random actions.
    gamma (float): The discount factor for future rewards in the Q-learning algorithm.
    memory (deque): The replay memory for storing past experiences.
    device (torch.device): The device (CPU or GPU) for running the neural network model.
    model (Linear_QNet): The neural network model for Q-learning.
    trainer (QTrainer): The trainer object for updating the model based on experiences.
    level (int): The current level of the AI agent.
    attempt (int): The number of attempts made to reach the next level.
    inventory (dict): The inventory of items held by the AI agent.

Methods:
    nb_item(looked, item): Counts the number of occurrences of an item in a string.
    get_state(response): Converts the server response into a state representation.
    check_level_up(box_zero): Checks if the AI agent can level up based on the current state.
    remember(state, action, reward, next_state, done): Stores an experience in the replay memory.
    train_long_memory(): Trains the model using experiences from the replay memory.
    train_short_memory(state, action, reward, next_state, done): Trains the model using a single experience.
    get_action(state): Chooses an action to perform based on the current state.
    compute_reward(conn, final_move, result, elapsed_time): Computes the reward for a given action and server response.
    add_item(response, command): Updates the inventory based on the server response and command.
    save_information(): Saves the AI agent's information to a file.
    train(conn): Trains the AI agent by interacting with the Zappy server.
\end{DoxyVerb}
 

\doxysubsection{Member Function Documentation}
\Hypertarget{classcollector__agent_1_1TuringAI_acb4b0925a5102cd09048259c36c98a98}\label{classcollector__agent_1_1TuringAI_acb4b0925a5102cd09048259c36c98a98} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!add\_item@{add\_item}}
\index{add\_item@{add\_item}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{add\_item()}{add\_item()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+add\+\_\+item (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{response,  }\item[{}]{command }\end{DoxyParamCaption})}

\begin{DoxyVerb}Adds or removes an item from the agent's inventory based on the response and command.

Args:
    response (str or bytes): The response received from the server.
    command (str): The command that was sent to the server.

Returns:
    None\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_ad0209978aa21f09800352db495c5e4b6}\label{classcollector__agent_1_1TuringAI_ad0209978aa21f09800352db495c5e4b6} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!check\_level\_up@{check\_level\_up}}
\index{check\_level\_up@{check\_level\_up}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{check\_level\_up()}{check\_level\_up()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+check\+\_\+level\+\_\+up (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{box\+\_\+zero }\end{DoxyParamCaption})}

\begin{DoxyVerb}Checks if the collector agent can level up based on the items in the given box.

Args:
    box_zero (list): The list of items in the collector agent's box.

Returns:
    bool: True if the collector agent can level up, False otherwise.
\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_a45d9c5b9867ca718d43260b0714edbdd}\label{classcollector__agent_1_1TuringAI_a45d9c5b9867ca718d43260b0714edbdd} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!compute\_reward@{compute\_reward}}
\index{compute\_reward@{compute\_reward}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{compute\_reward()}{compute\_reward()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+compute\+\_\+reward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{conn,  }\item[{}]{final\+\_\+move,  }\item[{}]{result,  }\item[{}]{elapsed\+\_\+time }\end{DoxyParamCaption})}

\begin{DoxyVerb}Computes the reward based on the final move, the result, and the elapsed time.

Args:
    conn (connection): The connection object used for communication.
    final_move (str): The final move made by the agent.
    result (str): The result received from the server.
    elapsed_time (float): The elapsed time for the current move.

Returns:
    int: The computed reward value.\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_a2010244c6386da9ea421ec83f3a8f2e5}\label{classcollector__agent_1_1TuringAI_a2010244c6386da9ea421ec83f3a8f2e5} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!get\_action@{get\_action}}
\index{get\_action@{get\_action}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{get\_action()}{get\_action()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+get\+\_\+action (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{state }\end{DoxyParamCaption})}

\begin{DoxyVerb}Returns the action to be taken by the collector agent based on the given state.

Args:
    state (list): The current state of the agent.

Returns:
    int: The index of the action to be taken.\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_ab58a7f359400374bb84c51476b9de022}\label{classcollector__agent_1_1TuringAI_ab58a7f359400374bb84c51476b9de022} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!get\_state@{get\_state}}
\index{get\_state@{get\_state}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{get\_state()}{get\_state()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+get\+\_\+state (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{response }\end{DoxyParamCaption})}

\begin{DoxyVerb}Get the state of the agent based on the response received.

Parameters:
- response (str): The response received from the server.

Returns:
- numpy.ndarray: The state of the agent, represented as a numpy array.

The state is determined by parsing the response and counting the number of items in the agent's inventory.
The response should be in the format: 'done' or '[item1:item1_count][item2:item2_count]...'

Example:
If the response is '[food:3][linemate:1][sibur:2]', the state will be [3, 1, 0, 2, 0, 0, 0, 3, 1, 0, 2, 0, 0, 0],
where the first 7 elements represent the counts of each item type, and the last 7 elements represent the agent's inventory.
\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_a10564f82ddc1142bf22a1c86244dea0b}\label{classcollector__agent_1_1TuringAI_a10564f82ddc1142bf22a1c86244dea0b} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!nb\_item@{nb\_item}}
\index{nb\_item@{nb\_item}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{nb\_item()}{nb\_item()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+nb\+\_\+item (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{looked,  }\item[{}]{item }\end{DoxyParamCaption})}

\begin{DoxyVerb}Counts the occurrences of a specific item in a comma-separated string.

Args:
    looked (str): A comma-separated string containing items.
    item (str): The item to count occurrences of.

Returns:
    list: A list containing the count of occurrences of the item in each substring.

Example:
    >>> agent = CollectorAgent()
    >>> agent.nb_item("apple,banana,apple,orange", "apple")
    [2, 0, 1, 0]
\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_a62495a46224207018b55804fb7e98768}\label{classcollector__agent_1_1TuringAI_a62495a46224207018b55804fb7e98768} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!remember@{remember}}
\index{remember@{remember}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{remember()}{remember()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+remember (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{state,  }\item[{}]{action,  }\item[{}]{reward,  }\item[{}]{next\+\_\+state,  }\item[{}]{done }\end{DoxyParamCaption})}

\begin{DoxyVerb}Stores the experience tuple (state, action, reward, next_state, done) in the agent's memory.

Parameters:
state (object): The current state of the agent.
action (object): The action taken by the agent.
reward (float): The reward received by the agent.
next_state (object): The next state after taking the action.
done (bool): Indicates whether the episode is done or not.
\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_a1c57e9bb8659005fa7519995e588644e}\label{classcollector__agent_1_1TuringAI_a1c57e9bb8659005fa7519995e588644e} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!save\_information@{save\_information}}
\index{save\_information@{save\_information}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{save\_information()}{save\_information()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+save\+\_\+information (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Save the agent's information to a file.

If the agent's level is 2, the information is saved in a file named
'LEVELUPinformation_{team_name}.txt' inside the 'saved_information' directory.
Otherwise, the information is saved in a file named 'information_{team_name}.txt'
inside the 'saved_information' directory.

The information includes the team name, attempt number, number of games played,
inventory, and level.

Returns:
    None
\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_a89ab9a530c27c52fc56021638410d5f3}\label{classcollector__agent_1_1TuringAI_a89ab9a530c27c52fc56021638410d5f3} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!train@{train}}
\index{train@{train}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{train()}{train()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+train (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{conn }\end{DoxyParamCaption})}

\begin{DoxyVerb}Trains the collector agent by interacting with the environment.

Args:
    conn: The connection object used to communicate with the environment.

Returns:
    None
\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_a0ec7d8b21bb7c978c3e365b9d51e1ccc}\label{classcollector__agent_1_1TuringAI_a0ec7d8b21bb7c978c3e365b9d51e1ccc} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!train\_long\_memory@{train\_long\_memory}}
\index{train\_long\_memory@{train\_long\_memory}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{train\_long\_memory()}{train\_long\_memory()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+train\+\_\+long\+\_\+memory (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Trains the agent's long-term memory by sampling a batch of experiences from the memory buffer and performing a training step.

If the memory buffer contains more experiences than the batch size, a random sample of size BATCH_SIZE is taken. Otherwise, the entire memory buffer is used.

The sampled experiences are then unpacked into separate lists for states, actions, rewards, next states, and dones. These lists are passed to the trainer's `train_step` method for training.

Parameters:
- None

Returns:
- None
\end{DoxyVerb}
 \Hypertarget{classcollector__agent_1_1TuringAI_af4930c63b53df4f4a801a7917b83e4c2}\label{classcollector__agent_1_1TuringAI_af4930c63b53df4f4a801a7917b83e4c2} 
\index{collector\_agent.TuringAI@{collector\_agent.TuringAI}!train\_short\_memory@{train\_short\_memory}}
\index{train\_short\_memory@{train\_short\_memory}!collector\_agent.TuringAI@{collector\_agent.TuringAI}}
\doxysubsubsection{\texorpdfstring{train\_short\_memory()}{train\_short\_memory()}}
{\footnotesize\ttfamily collector\+\_\+agent.\+Turing\+AI.\+train\+\_\+short\+\_\+memory (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{state,  }\item[{}]{action,  }\item[{}]{reward,  }\item[{}]{next\+\_\+state,  }\item[{}]{done }\end{DoxyParamCaption})}

\begin{DoxyVerb}Trains the agent's short-term memory by passing the state, action, reward, next_state, and done flag to the trainer.

Parameters:
state (object): The current state of the agent.
action (object): The action taken by the agent.
reward (float): The reward received by the agent.
next_state (object): The next state of the agent.
done (bool): A flag indicating if the episode is done.

Returns:
None
\end{DoxyVerb}
 

The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
ia/collector\+\_\+agent.\+py\end{DoxyCompactItemize}
